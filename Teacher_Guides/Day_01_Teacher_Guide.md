# Day 1 Teacher's Guide
## Evening 1: Signal vs. Noise - Cutting Through the AI Hype

**Course:** AI for Software Managers Workshop  
**Session Date:** January 20, 2026  
**Location:** Digital Campus Vorarlberg  
**Instructor:** Frank Blau  
**Duration:** Evening Session (3-4 hours recommended)

---

## Table of Contents
1. [Pre-Class Setup Instructions](#pre-class-setup-instructions)
2. [Learning Objectives](#learning-objectives)
3. [Tool and Software Requirements](#tool-and-software-requirements)
4. [Daily Schedule](#daily-schedule)
5. [Detailed Topic Guide](#detailed-topic-guide)
6. [Activities and Exercises](#activities-and-exercises)
7. [Assignments](#assignments)
8. [Assessment and Evaluation](#assessment-and-evaluation)

---

## Pre-Class Setup Instructions

### 48 Hours Before Class

**Instructor Preparation:**
- [ ] Review all materials in the student workbook
- [ ] Familiarize yourself with the AI Tool Evaluation Matrix framework
- [ ] Prepare real-world case studies of AI tool implementations (2-3 examples)
- [ ] Test all technology systems (projector, internet, video clips if used)
- [ ] Print backup copies of handouts and exercises
- [ ] Create a shared folder/document repository for class materials

**Room Setup:**
- [ ] Arrange chairs and tables to facilitate group discussion
- [ ] Ensure internet connectivity is stable and tested
- [ ] Set up projector and presentation system
- [ ] Verify audio/visual equipment works properly
- [ ] Arrange for breakout spaces for small group work (if possible)
- [ ] Have whiteboards or flip charts available for capturing key ideas
- [ ] Ensure adequate lighting and comfortable room temperature

**Materials Preparation:**
- [ ] Print one "AI Tool Evaluation Matrix" worksheet per participant
- [ ] Print one "ROI Calculator Template" worksheet per participant
- [ ] Print one "Risk Assessment Template" worksheet per participant
- [ ] Print one "Stakeholder Alignment Guide" worksheet per participant
- [ ] Prepare handout with Common AI Tool Categories (Code Completion, Analysis, Documentation, Testing)
- [ ] Create a resource list with links to tools mentioned in the workbook
- [ ] Prepare a "Reality Check Framework" reference card

### 24 Hours Before Class

**Participant Communications:**
- [ ] Send reminder email with class details
- [ ] Include parking/transportation information
- [ ] Request participants bring laptops (optional but recommended)
- [ ] Ask participants to think about one AI tool they want to evaluate
- [ ] Provide WiFi network credentials and password

**Last-Minute Checks:**
- [ ] Verify all handouts are printed and organized
- [ ] Test presentation slides/materials one final time
- [ ] Prepare example evaluations using the AI Tool Evaluation Matrix
- [ ] Set up recording system (if capturing session for review)

### Day of Class (Arrive 30 minutes early)

**Room Verification:**
- [ ] Test all AV equipment one final time
- [ ] Check room temperature and lighting
- [ ] Arrange materials at tables or entry point for distribution
- [ ] Set up registration/sign-in if needed
- [ ] Verify WiFi is broadcasting and functioning
- [ ] Test internet speed and stability

**Technical Setup:**
- [ ] Open presentation materials and verify all display properly
- [ ] Test video playback if any are included
- [ ] Have backup materials accessible (USB drive, cloud access)
- [ ] Verify chat/collaboration tools are active if using them

---

## Learning Objectives

By the end of Day 1, participants will be able to:

### Knowledge Objectives
- **Distinguish** between realistic AI capabilities and hype around AI tools
- **Explain** what different categories of AI tools can and cannot do
- **Identify** the key evaluation criteria for selecting appropriate AI tools for their organization
- **Understand** the business case for AI adoption, including ROI considerations
- **Recognize** the primary risks associated with AI tool implementation
- **Articulate** the importance of stakeholder alignment in AI adoption

### Skill Objectives
- **Apply** the Reality Check Framework to evaluate AI claims in marketing materials
- **Use** the AI Tool Evaluation Matrix to systematically assess tools against organizational criteria
- **Calculate** basic ROI projections using the provided template
- **Analyze** risks using the provided Risk Assessment Templates
- **Develop** initial stakeholder communication strategies using the Alignment Guide

### Attitude Objectives
- **Adopt** a balanced, analytical approach to evaluating AI tools (avoiding both hype and dismissal)
- **Appreciate** the complexity of AI adoption beyond just tool selection
- **Commit** to evidence-based decision-making regarding AI investments
- **Recognize** the human factors (organizational readiness, stakeholder concerns) in successful adoption

---

## Tool and Software Requirements

### Required Software/Platforms

| Tool | Version | Purpose | Validation |
|------|---------|---------|-----------|
| Spreadsheet Software | Excel 2019+ or Google Sheets | ROI calculations, matrix scoring | Test: Verify all formulas work correctly before class |
| Web Browser | Chrome/Firefox/Safari (latest) | Accessing AI tool websites, research | Test: Verify can access evaluation tool websites |
| PDF Viewer | Built-in (Preview/Adobe) | Viewing reference materials | Test: Open sample PDFs included in materials |
| Video Conferencing (Optional) | Zoom/Teams | For hybrid/remote participants | Test: Verify audio/video quality 24h before |
| Collaboration Tool (Optional) | Miro, Mural, or Google Docs | Shared note-taking/brainstorming | Test: Create sample board/doc and test sharing |

### Hardware Requirements

| Item | Specification | Notes |
|------|---------------|-------|
| Projector | 2500+ lumens minimum | Ensure it handles 1920x1080 resolution |
| Screen/Whiteboard | 60"+ recommended | For visibility from back of room |
| Audio System | Microphone + speakers | Test audio levels before participants arrive |
| Laptops | For participants (optional) | WiFi connection required; personal devices OK |
| Tablets/Notebooks | For note-taking | Paper notebooks acceptable alternative |

### Connectivity Requirements

- **Internet Bandwidth:** 10+ Mbps minimum (for research during exercises)
- **WiFi Coverage:** Test signal in all areas of classroom
- **Network Security:** Ensure corporate firewall allows access to evaluation tools
- **Backup Connectivity:** Have mobile hotspot as backup internet source

### Reference Materials to Have Ready

1. **Printed Handouts:**
   - AI Tool Evaluation Matrix (completed examples)
   - ROI Calculator Template (sample calculations)
   - Reality Check Framework cheat sheet
   - List of Common AI Tools with brief descriptions

2. **Digital Resources:**
   - Links to GitHub Copilot, ChatGPT, Claude, Amazon CodeWhisperer
   - Case studies of AI tool adoption successes and failures
   - Security and privacy policy templates
   - Industry benchmarks for ROI expectations

3. **Assessment Tools:**
   - Opening assessment questions (page 49-52 of workbook)
   - Post-session feedback survey template
   - Homework assignment checklist

---

## Daily Schedule

### Recommended Session Structure (3-4 hours)

| Time | Duration | Activity | Materials | Notes |
|------|----------|----------|-----------|-------|
| 6:00 PM | 10 min | Welcome & Icebreaker | None | Set tone: balanced, practical approach |
| 6:10 PM | 10 min | Learning Objectives & Agenda | Projector | Show what they'll accomplish tonight |
| 6:20 PM | 15 min | Opening Assessment Exercise | Workbook, pencils | Individual reflection on org readiness |
| 6:35 PM | 20 min | Reality Check Framework Presentation | Slides, handouts | Break down what AI can/cannot do |
| 6:55 PM | 15 min | AI Tool Categories Deep Dive | Reference cards | Discussion of 4 categories with examples |
| 7:10 PM | **10 min** | **BREAK** | Refreshments | Bathroom break, network, informal questions |
| 7:20 PM | 30 min | AI Tool Evaluation Matrix Intro | Matrices, examples | Walk through scoring system, discuss weights |
| 7:50 PM | 20 min | Group Exercise: Evaluate Sample Tool | Whiteboard, markers | Live evaluation of real tool (e.g., GitHub Copilot) |
| 8:10 PM | 20 min | ROI Calculator & Risk Assessment | Calculators, templates | Show how to calculate ROI and identify risks |
| 8:30 PM | 15 min | Stakeholder Alignment Guide | Guide handout | Discussion: who needs to be involved? |
| 8:45 PM | 30 min | Small Group Exercise: Tool Evaluation | Matrices, laptops | Groups evaluate tool of interest (from workbook) |
| 9:15 PM | 15 min | Group Sharing & Q&A | Whiteboard | Each group shares findings (5 min per group) |
| 9:30 PM | 10 min | Homework Assignment Review | Assignment sheet | Explain homework, answer questions |
| 9:40 PM | 5 min | Closing Remarks & Contact Info | None | Set expectations for Day 2, exchange contact info |

**Total Recommended Duration:** 3 hours 40 minutes  
**Flexible Options:** Can be condensed to 3 hours by reducing breaks and group sharing time

---

## Detailed Topic Guide

### Topic 1: Opening Assessment & Context Setting (25 minutes)

**Objective:** Understand participants' baseline knowledge and organizational readiness

**Key Points to Convey:**
- This is a practical workshop, not a technology deep-dive
- Focus is on making good decisions about AI adoption
- There are no "wrong" answers in the opening assessment
- AI adoption is about people, process, and tools (in that order)

**Detailed Content Outline:**

**A. Welcome & Community Norms (5 minutes)**
- Introduce yourself and brief background
- Ground rules: "This is a safe space for honest questions"
- Reinforce: "AI hype is realâ€”our job is to cut through it"
- Encourage note-taking and questions throughout

**B. Opening Assessment Exercise (15 minutes)**
- Have participants individually complete the Opening Assessment (pages 49-52 of workbook)
- Areas to assess:
  - AI Awareness (management, developers, executive support)
  - Current AI Usage (tools in use, budget allocated)
  - Organizational Readiness (policies, code review processes, training)
  - Challenges faced
  - Goals for workshop
- Collect these (with names optional) to understand group profile
- Use to tailor examples and depth during remaining sessions

**C. Group Debrief (5 minutes)**
- Ask 2-3 volunteers to share one challenge their organization faces
- Affirm that all challenges are common and addressable
- Note: Will address all these concerns over the next three evenings

**Discussion Prompts:**
- "How many of you already have developers using AI tools informally?"
- "What's your biggest concern about AI adoption?"
- "Why is this workshop important to you right now?"

---

### Topic 2: The Reality Check Framework (35 minutes)

**Objective:** Build shared understanding of what AI tools actually do well and where they have limitations

**Why This Matters:**
- Hype is the biggest barrier to good decision-making
- Marketing materials rarely mention limitations
- Setting realistic expectations prevents failure and disappointment
- Helps team members make credible recommendations

**Key Teaching Approach:**
- Use concrete examples from their industry when possible
- Encourage healthy skepticism
- Emphasize: "AI is a tool, not a solution"
- Show real examples of both successes and failures

**Detailed Content Outline:**

**A. What AI Tools Can Do Well (15 minutes)**

1. **Generate code from clear specifications**
   - Example: "Write a Python function that validates email addresses"
   - Limitation: Requires clear specification upfront
   - Teaching note: Show before/after of poor vs. clear specifications

2. **Suggest completions for repetitive patterns**
   - Example: Loop structures, standard error handling
   - Limitation: Assumes common patterns; fails on novel problems
   - Real example: GitHub Copilot excels at CRUD operations

3. **Explain existing code**
   - Example: Paste legacy code, ask AI to explain
   - Limitation: May oversimplify or misunderstand intent
   - Use case: Great for code reviews and onboarding

4. **Translate between programming languages**
   - Example: Python to JavaScript translation
   - Limitation: Doesn't understand context-specific idioms
   - Caution: Always review translated code carefully

5. **Generate test cases**
   - Example: "Generate edge case tests for this function"
   - Limitation: May miss business-critical edge cases
   - Best practice: Use as starting point, always review and enhance

6. **Document code automatically**
   - Example: Generate docstrings from function signatures
   - Limitation: Often too generic or technically focused
   - Value: Saves time on boilerplate, improves consistency

7. **Find common bugs and security issues**
   - Example: SQL injection vulnerabilities, null reference errors
   - Limitation: Misses complex or novel vulnerabilities
   - Recommendation: Use alongside traditional security scanning

**B. What AI Tools Cannot Do (Yet) (10 minutes)**

1. **Understand complex business requirements without context**
   - Why: AI lacks domain knowledge and business context
   - Impact: May suggest technically correct but wrong solution
   - Mitigation: Managers must provide clear requirements

2. **Make architectural decisions**
   - Why: Requires understanding of tradeoffs, scalability, maintenance
   - Teaching point: THIS is where human expertise is most valuable
   - Example: When to use microservices vs. monolith

3. **Evaluate tradeoffs between different approaches**
   - Why: Requires business judgment, not just technical evaluation
   - Reality: AI can generate alternatives; humans choose the right one
   - Implication: Architects remain essential

4. **Debug complex system interactions**
   - Why: Requires understanding of distributed systems, dependencies
   - Example: Multi-service performance issues
   - Recommendation: AI can help with single-service debugging only

5. **Understand your specific codebase without examples**
   - Why: Each organization has unique conventions, patterns, anti-patterns
   - Teaching note: "Give AI examples to teach it your patterns"
   - Practical: First few uses may be lower quality until AI learns your codebase

6. **Replace human judgment and creativity**
   - Why: AI optimizes within known patterns; can't break new ground
   - Example: Novel algorithm design, innovative UI/UX
   - Reality: Best solutions combine human creativity + AI efficiency

7. **Maintain context across large codebases**
   - Why: Context window limitations and lack of true understanding
   - Challenge: Works well on files, struggles with 100K+ line systems
   - Workaround: Break work into smaller, scoped tasks

**C. Key Teaching Insight to Emphasize:**
"AI is best at:
- **Acceleration:** Doing faster what we already know how to do
- **Exploration:** Showing alternatives we might not have thought of
- **Refinement:** Taking good code and making it better

AI is worst at:
- **Innovation:** Creating entirely new approaches
- **Context:** Understanding the big picture
- **Judgment:** Deciding what's right for your situation"

---

### Topic 3: Common AI Tool Categories (20 minutes)

**Objective:** Help participants understand the landscape of AI tools available

**Why This Matters:**
- Different tools serve different purposes
- Choosing the right category prevents waste and frustration
- Managers need to know what options exist

**Detailed Content Outline:**

**A. Code Completion and Generation (5 minutes)**

**Definition:** Tools that suggest code completions, generate code from comments, or create functions

**Examples:**
- GitHub Copilot (VS Code, JetBrains IDEs)
- Amazon CodeWhisperer (AWS, IDEs)
- Tabnine (multiple IDEs)
- Replit Ghostwriter

**Best For:**
- Writing boilerplate code (getters, setters, common patterns)
- Standard function implementations
- Exploratory coding when learning new frameworks
- Speed in straightforward coding tasks
- Learning new programming languages

**Limitations:**
- May not understand your specific architecture
- Assumes common patterns; struggles with novel code
- Quality depends heavily on comments/context provided
- Can be unpredictable with complex requirements
- May perpetuate poor patterns if not reviewed

**Real-World Impact:** Can reduce time on routine coding by 20-40%

**Teaching Example:** Show a simple REST endpoint being generated; then show a failure with complex business logic

---

**B. Code Analysis and Review (4 minutes)**

**Definition:** Tools that analyze code for bugs, security issues, code quality

**Examples:**
- SonarQube with AI extensions
- DeepCode (now part of Snyk)
- GitHub Code Scanning (with AI)
- Amazon CodeGuru

**Best For:**
- Finding security vulnerabilities early
- Identifying code quality issues
- Suggesting performance improvements
- Spotting common bug patterns
- Standards compliance checking

**Limitations:**
- Cannot evaluate business logic correctness
- High false-positive rates possible
- Only works on code that can be scanned (not compiled requirements)
- Cannot understand intent, only implementation

**Real-World Impact:** Reduces bugs reaching production by 15-25%

**Teaching Example:** Show a real security issue that was caught; then discuss false positive scenario

---

**C. Documentation and Explanation (4 minutes)**

**Definition:** General-purpose AI that can explain code, generate documentation, answer questions

**Examples:**
- ChatGPT (general purpose, but code-capable)
- Claude (Anthropic, strong at technical explanation)
- GitHub Copilot Chat (IDE-integrated conversation)
- Gemini (Google's general-purpose AI)

**Best For:**
- Explaining existing code during code reviews
- Generating documentation and README files
- Answering technical questions
- Learning how to do something new
- Creating comments and docstrings
- Translating between languages

**Limitations:**
- May misunderstand complex or unusual code
- Explanations can be overly simplified
- Not IDE-integrated (context switching)
- Requires copy-pasting code into chat
- May hallucinate or be confidently wrong

**Real-World Impact:** Saves 2-4 hours per week on documentation and explanation

**Teaching Example:** Ask Claude to explain a complex algorithm; show both accurate and inaccurate responses

---

**D. Testing (3 minutes)**

**Definition:** Tools specifically designed for generating and managing test code

**Examples:**
- Sapienz (Facebook)
- EvoSuite (research tool)
- GitHub Copilot for test generation
- General-purpose AIs for test logic
- AI-powered test frameworks

**Best For:**
- Creating unit test skeleton code
- Identifying edge cases to test
- Generating test data
- Regression test creation
- Test documentation

**Limitations:**
- Cannot understand intended behavior without guidance
- May miss business-critical test scenarios
- Generated tests may not be comprehensive
- Requires human verification of test correctness
- Best used as starting point, not complete solution

**Real-World Impact:** Accelerates test writing by 30-40%; quality depends on guidance

**Teaching Example:** Show AI generating tests; discuss what it missed and why

---

### Topic 4: The AI Tool Evaluation Matrix (30 minutes)

**Objective:** Teach participants how to systematically evaluate tools against organizational criteria

**Why This Matters:**
- Ad-hoc tool selection leads to waste and poor adoption
- Organizations need consistent evaluation criteria
- Matrix approach removes emotion from decision-making
- Creates audit trail for compliance

**Detailed Content Outline:**

**A. Introduction to Structured Evaluation (5 minutes)**

"Why Matrices Matter:"
- Removes vendor hype from consideration
- Ensures all stakeholders use same criteria
- Creates defensible decision documentation
- Enables comparison across multiple tools
- Reveals tradeoffs explicitly

"When to Use This Framework:"
- First-time tool evaluation
- Significant budget decisions
- Tools with organizational impact
- Risk assessment required
- Vendor lock-in potential

---

**B. The Five Evaluation Categories (20 minutes)**

**Category 1: Capabilities (Weight: 25%)**

*Why Important: Does it actually do what we need?*

Criteria to assess:
1. **Addresses our specific needs** (Rating 1-5)
   - Does it solve our primary problem?
   - Guides: List your top 3 needs first
   - Example: "We need to reduce code review time" (not generic capability)

2. **Works with our tech stack** (Rating 1-5)
   - Language support? Framework compatibility?
   - Example: Does it support Python/Django? (your stack)
   - Teaching tip: Non-negotiable for code tools

3. **Integration with existing tools** (Rating 1-5)
   - Works with our IDE?
   - Integrates with CI/CD pipeline?
   - Works with our repository system?
   - Teaching example: GitHub Copilot + VS Code = 5; ChatGPT alone = 2 for same task

4. **Quality of output** (Rating 1-5)
   - Do test implementations show reliable results?
   - How much review/fixing is needed?
   - Documentation accuracy?
   - Teaching tip: Do proof-of-concept before full commitment

5. **Ease of use** (Rating 1-5)
   - Does it require extensive training?
   - Learning curve manageable?
   - Intuitive interface?
   - Teaching example: "Installation took 20 minutes vs. 20 hours"

**Category Score:** Sum of individual criteria (max 25 points)

**Teaching Guidance:**
- Capabilities are most important (highest weight)
- Don't prioritize cool features; prioritize actual needs
- Be honest about fit; no tool is perfect
- Common mistake: Rating 5 because vendor claimed it, not because tested

---

**Category 2: Cost and Value (Weight: 20%)**

*Why Important: Can we afford it? Will we get value?*

Criteria to assess:
1. **Pricing clarity** (Rating 1-5)
   - Price clearly stated?
   - Per-seat? Per-organization? Usage-based?
   - Hidden fees?
   - Teaching point: If pricing is unclear, that's a red flag

2. **Cost per developer reasonable** (Rating 1-5)
   - Industry benchmark comparison
   - Can we afford to scale to all teams?
   - Teaching example: $20/month reasonable; $200/month requires strong justification

3. **Clear ROI potential** (Rating 1-5)
   - Can quantify savings/benefits?
   - Realistic timeline to positive ROI?
   - Measurable outcomes?
   - Teaching example: "20% productivity gain" (specific) vs. "faster" (vague)

4. **Free trial available** (Rating 1-5)
   - Can evaluate before committing?
   - Trial reflects production use case?
   - Adequate trial duration?
   - Teaching point: No trial = higher risk

**Category Score:** Sum of individual criteria (max 20 points)

**Teaching Guidance:**
- ROI doesn't have to be immediate, but must be visible
- Watch for "free tier forever" vendors (may not scale)
- Total cost includes training, not just licensing
- Hidden cost: Vendor lock-in and switching costs

---

**Category 3: Security and Privacy (Weight: 25%)**

*Why Important: Highest weight because this is existential risk*

Criteria to assess:
1. **Data privacy controls** (Rating 1-5)
   - What data does tool collect?
   - Can we audit what's collected?
   - GDPR/compliance certified?
   - Teaching point: This is where most concerns arise

2. **Code confidentiality** (Rating 1-5)
   - Is our code used for training the AI?
   - Can code be shared with other customers?
   - Can code be seen by vendor staff?
   - Teaching example: GitHub Copilot shares with Copilot but not training by default
   - Teaching example: ChatGPT default WILL use your input for training

3. **Compliance certifications** (Rating 1-5)
   - SOC 2? ISO 27001? HIPAA? FedRAMP?
   - Matches your compliance needs?
   - Third-party audited?
   - Teaching tip: Must match YOUR requirements, not assume one size fits all

4. **On-premise option available** (Rating 1-5)
   - For sensitive situations, can we run locally?
   - Open-source option?
   - Self-hosted option?
   - Teaching note: Not always needed, but valuable option

5. **Clear terms of service** (Rating 1-5)
   - Legal review possible?
   - Acceptable use policy clear?
   - Liability limitations understood?
   - Teaching point: Have legal review before signing

**Category Score:** Sum of individual criteria (max 25 points)

**Teaching Guidance:**
- Security is weighted equally with capabilities
- This is THE differentiator between tools
- "Trust us" is not acceptable answer
- Most organizations accept some risk, but it must be documented and acceptable
- Common mistake: Prioritizing cost over security

---

**Category 4: Vendor Stability (Weight: 15%)**

*Why Important: You need the vendor to stay in business*

Criteria to assess:
1. **Company track record** (Rating 1-5)
   - How long in business?
   - History of innovation?
   - Reputation in market?
   - Teaching example: Google (stable) vs. startup (risky)

2. **Customer base size** (Rating 1-5)
   - Enterprise customers?
   - Scale of deployment?
   - Customer retention?
   - Teaching point: Size = stability signal

3. **Financial stability** (Rating 1-5)
   - Profitability or well-funded?
   - Market conditions?
   - Recent funding status?
   - Teaching example: Post-funding = good signal; needing another round = potential concern

**Category Score:** Sum of individual criteria (max 15 points)

**Teaching Guidance:**
- Lower weight because less critical than capabilities/security
- But catastrophic if vendor goes out of business
- Watch for: Multiple layoffs, leadership changes, dramatic pivots
- Due diligence: Check CrunchBase, company news, ask references

---

**Category 5: Support and Documentation (Weight: 15%)**

*Why Important: Tools fail; support quality matters*

Criteria to assess:
1. **Documentation quality** (Rating 1-5)
   - Is documentation current and accurate?
   - Examples provided?
   - Clear onboarding guides?
   - Teaching tip: Try following docs before committing

2. **Support responsiveness** (Rating 1-5)
   - What support levels available?
   - SLA for response time?
   - Paid vs. free support?
   - Teaching example: Community-only (slow) vs. 24/7 enterprise support

3. **Community and resources** (Rating 1-5)
   - Active community forums?
   - YouTube tutorials?
   - Third-party training?
   - Teaching point: Community is backup to commercial support

**Category Score:** Sum of individual criteria (max 15 points)

**Teaching Guidance:**
- Support matters more after purchase
- Weak documentation often means weak product
- Community support is great but not substitute for vendor support
- Factor in: Time for staff to find/apply solutions

---

**C. Calculating the Overall Score (3 minutes)**

**Scoring Formula:**

1. **Score each criterion:** 1-5 scale
   - 1 = Not acceptable / Critical gaps
   - 2 = Below expectations
   - 3 = Adequate / Meets basic needs
   - 4 = Good / Exceeds expectations
   - 5 = Excellent / Best-in-class

2. **Sum within each category:** (total depends on number of criteria)

3. **Calculate weighted total:**
   - (Capabilities Score / 25 Ã— 0.25) = Weighted Capabilities Score
   - (Cost Score / 20 Ã— 0.20) = Weighted Cost Score
   - (Security Score / 25 Ã— 0.25) = Weighted Security Score
   - (Vendor Score / 15 Ã— 0.15) = Weighted Vendor Score
   - (Support Score / 15 Ã— 0.15) = Weighted Support Score
   
   **Total Score = Sum of all weighted scores (0-100)**

**Interpretation of Scores:**
- **80-100: Strong candidate** â†’ Proceed with pilot
- **60-79: Potential fit** â†’ Needs more evaluation; address gaps
- **40-59: Significant concerns** â†’ Evaluate alternatives; address before proceeding
- **Below 40: Not recommended** â†’ Significant risks or poor fit

---

### Topic 5: ROI Calculator & Quick Risk Overview (25 minutes)

**Objective:** Help managers make business case for AI investment

**A. ROI Calculator Framework (15 minutes)**

**Why This Matters:**
- "ROI" is most important question for executives
- Most AI tool vendors provide unrealistic numbers
- Organizations need credible, conservative estimates
- This helps validate whether pilot is worth doing

**Step 1: Calculate Costs (10 minutes)**

Tool Costs (Annual):
- License fees per developer Ã— number of developers
- Example: $20/month Ã— 50 developers Ã— 12 months = $12,000
- Teaching tip: Don't forget to scale out (all developers, not pilot group)

Infrastructure costs:
- Server/hosting requirements
- VPN/security requirements
- Example: Usually minimal, but check

Training costs:
- Initial training program
- Ongoing training for new hires
- Example: $5,000-$20,000 for initial training
- Teaching tip: This is often underestimated

Implementation time (hours Ã— cost per hour):
- Setup, configuration, integration
- Example: 100 hours Ã— $100/hour = $10,000
- Teaching note: Include your people, not just vendor

**Total Annual Cost Example:**
- Licenses: $12,000
- Infrastructure: $1,000
- Training: $10,000
- Implementation: $10,000
- **Total: $33,000**

---

**Step 2: Calculate Expected Benefits (15 minutes)**

**Time Savings:**
- Average hours saved per developer per week
  - Teaching example: "2 hours per week" is reasonable start
  - Conservative: Many tools promise 5-10 hours; reality is 1-3
  - Varies widely by task type and developer skill
- Calculation: Hours saved Ã— developers Ã— weeks Ã— hourly rate
- Example: 2 hrs/week Ã— 50 devs Ã— 50 weeks Ã— $50/hr = $250,000/year
- Teaching caution: This is the biggest number; be conservative!

**Quality Improvements:**
- Estimated bug reduction percentage
  - Teaching example: "10% reduction in production bugs"
  - Data: Varies by tool; range is 5-15% typically
- Calculation: Bug reduction % Ã— bugs/year Ã— cost per bug
- Example: 10% Ã— 500 bugs Ã— $200 per bug = $10,000/year
- Teaching note: Cost per bug includes debugging time, customer impact, reputation

**Faster Delivery:**
- Estimated project acceleration percentage
  - Teaching example: "5% faster project completion"
  - Value: Earlier revenue, faster feature delivery
- Calculation: Acceleration % Ã— value of faster delivery
- Example: 5% Ã— $2M annual revenue = $100,000/year
- Teaching caution: This is often overestimated; be conservative

**Total Annual Benefits Example:**
- Labor savings: $250,000
- Bug reduction: $10,000
- Faster delivery: $50,000
- **Total: $310,000**

---

**Step 3: Calculate ROI (5 minutes)**

**ROI Formula:**
```
ROI = (Total Benefits - Total Costs) / Total Costs Ã— 100
ROI = ($310,000 - $33,000) / $33,000 Ã— 100 = 839%
```

**Payback Period:**
```
Payback Period = Total Costs / (Monthly Benefits)
Payback Period = $33,000 / ($310,000/12) = 1.3 months
```

**Teaching Examples:**

"Scenario A: GitHub Copilot in dev shop"
- Costs: $33,000/year
- Benefits: $250,000 savings
- ROI: 657% | Payback: 1.6 months
- Recommendation: Proceed with pilot

"Scenario B: Expensive AI platform"
- Costs: $150,000/year
- Benefits: $180,000 savings
- ROI: 20% | Payback: 10 months
- Recommendation: More evaluation needed

"Scenario C: Budget overrun scenario"
- Costs: $200,000/year (expected: $50,000)
- Benefits: $150,000 savings
- ROI: -25% | Payback: Never
- Recommendation: DO NOT PROCEED

---

**Reality Check Questions for ROI (Teaching Points):**
- Are the time savings realistic? (Consider learning curve)
- Will all developers achieve the same productivity gains?
- Have you accounted for productivity loss during adoption? (Usually 2-4 weeks)
- Are there hidden costs? (License compliance, security audits, customization)
- Is the quality improvement estimate conservative? (Reality often 50% of projections)
- What's your confidence level in these numbers? (20%, 50%, 80%?)

---

**B. Risk Assessment Overview (10 minutes)**

**Why This Matters:**
- Benefits are uncertain; risks are real
- Need to identify and mitigate risks proactively
- Prevents "surprise" failures

**Key Risk Categories:**

**Technical Risks:**
- Tool integration failures (incompatible with your systems)
- Generated code quality problems (requires extensive review)
- Performance impact on development environment (slows IDE)
- Tool lacks features for your tech stack (tool doesn't support your languages)

**Financial Risks:**
- ROI doesn't materialize (fewer time savings than expected)
- Unexpected cost increases (per-seat charges balloon)
- Vendor changes pricing model (tier structure becomes expensive)
- Required infrastructure upgrades (security, compliance requires new systems)

**Organizational Risks:**
- Developer resistance ("I'll never use this")
- Loss of critical skills ("Now developers can't code without AI")
- Over-reliance on AI ("Nobody reviews the code anymore")
- Uneven adoption across teams (only some teams benefit)

**Security/Privacy Risks:**
- Code leaked to training data (your proprietary code exposed)
- Intellectual property concerns (who owns AI-generated code?)
- Compliance violations (tool doesn't meet regulatory requirements)
- Security vulnerabilities (AI generates insecure code)

**Teaching Guidance for Risk Assessment:**
"Use a simple 3-step process for each risk:
1. **Likelihood (1-5):** How likely is this to happen?
2. **Impact (1-5):** How bad would it be if it happened?
3. **Mitigation:** What can we do to prevent or reduce this?

High-priority = High likelihood Ã— High impact
Low-priority = Low likelihood or Low impact"

---

### Topic 6: Stakeholder Alignment (15 minutes)

**Objective:** Help managers develop stakeholder communication strategy

**Why This Matters:**
- AI adoption requires buy-in from multiple groups
- Each stakeholder has different concerns
- Poor communication leads to resistance and failure
- Executives, developers, and legal have opposite concerns

**Detailed Content Outline:**

**A. Identifying Key Stakeholders (5 minutes)**

**Executive Level** (focus: ROI, risk, competitive advantage)
- CFO: Cares about costs and ROI
- CEO: Cares about competitive advantage
- CTO: Cares about technical viability
- Chief Risk Officer: Cares about security and compliance
- Communications: Cares about external messaging

**Management Level** (focus: implementation, team impact)
- Engineering managers: Care about team productivity
- Product managers: Care about faster delivery
- QA managers: Care about code quality
- Security managers: Care about vulnerabilities

**Developer Level** (focus: tools, skills, job impact)
- Early adopters: Excited about new tools
- Skeptics: Worried about job security and code quality
- Team leads: Worried about team dynamics
- Architects: Worried about system design decisions

**Other Stakeholders**
- Legal: IP ownership, compliance
- Procurement: Vendor evaluation, contracting
- HR: Training, skill development implications

---

**B. Tailored Communication for Each Stakeholder (8 minutes)**

**For Executives:**
- Key messages:
  - Competitive advantage potential ("Our developers will be 20% more productive")
  - ROI and cost considerations ("Payback in 2 months, ROI 600%")
  - Risk mitigation approach ("We're piloting with low-risk team first")
  - Timeline and milestones ("Pilot: Jan-Feb, decision: March, full rollout: April")
- Preferred formats: Presentation, one-pager email, quarterly business review
- Frequency: Monthly updates during pilot

**For Developers:**
- Key messages:
  - How this helps them ("Less time on boilerplate, more on interesting work")
  - Training and support available ("Full training provided, no pressure to use")
  - Pilot approach ("Not forced adoption; we want your feedback")
  - Feedback mechanisms ("Your input shapes how we implement this")
- Preferred formats: Team meeting, live demo, written guide, Q&A session
- Frequency: Bi-weekly during pilot

**For Legal/Security:**
- Key messages:
  - Data privacy measures ("Code doesn't leave our firewall unless encrypted")
  - Compliance considerations ("Tool is SOC 2 certified")
  - Security controls ("All AI code goes through security scanning")
  - Vendor evaluation criteria ("We'll evaluate before committing")
- Preferred formats: Detailed report, security review meeting, policy document
- Frequency: Before commit, quarterly audit thereafter

**For HR:**
- Key messages:
  - Skill development opportunities ("New prompt engineering skills")
  - No immediate layoffs ("AI augments, doesn't replace")
  - Training programs ("Budget allocated for skill development")
  - Career paths ("Architecture and design roles more valuable than ever")
- Preferred formats: Training plan presentation, workshop invitation
- Frequency: One-time at launch, annual update

---

**C. Addressing Common Objections (2 minutes)**

**Teaching Script - Objection Handling:**

**Objection: "We cannot afford it"**
- Response: "The cost is $X per developer annually. Our estimated ROI is Y%, so we break even in Z months. Let's discuss whether these numbers are realistic for our situation."
- Teaching tip: Provide hard numbers, not dismissal

**Objection: "Our developers will lose their skills"**
- Response: "That's a valid concern. The reality is AI makes routine work faster, but complex work becomes MORE important. We'll invest in skills development so your team handles the complex problems that AI can't."
- Teaching tip: Acknowledge concern; provide specific mitigation

**Objection: "AI-generated code is not secure"**
- Response: "That's a legitimate concern. We're implementing security scanning of all AI-generated code, and security team will review sensitive areas. Let's pilot in low-risk areas first."
- Teaching tip: Don't dismiss; show you've thought about it

**Objection: "This is just a fad"**
- Response: "AI tools are evolving rapidly. Our approach is to pilot, measure results, and make data-driven decisions. We're not betting the company on AI, but we need to understand it."
- Teaching tip: Balanced perspective; acknowledge uncertainty

---

## Activities and Exercises

### Exercise 1: Opening Assessment (15 minutes)

**Location in Workbook:** Pages 49-52  
**Timing:** Early in session (20 minutes into class)  
**Individual vs. Group:** Individual

**Setup:**
- Distribute workbook or printed worksheets
- Explain: "This is not a test; it's a baseline assessment"
- Assure: Responses are confidential (collect anonymously if preferred)

**Instructions to Participants:**
1. Complete all sections individually (10 minutes)
   - AI Awareness section
   - Current AI Usage section
   - Organizational Readiness section
   - Challenges You're Facing section
   - Goals for This Workshop section
2. Be honest; there are no "right" answers
3. Raise hand with clarification questions only

**Instructor Role During Exercise:**
- Circulate to answer questions
- Note interesting patterns or concerns
- Do NOT collect papers yet

**Debrief (5 minutes):**
- Ask: "Would anyone like to share one challenge your organization faces?"
- Listen for patterns
- Affirm: "These are common challenges; we'll address each one over the three days"
- Transition: "Let's look at these challenges through the lens of what AI can and can't do"

**Use After Session:**
- Collect papers (optional: with or without names)
- Use to tailor Day 2 and Day 3 content
- Reference specific challenges when covering topics

---

### Exercise 2: Reality Check Framework (15 minutes)

**Location in Workbook:** Pages 74-88  
**Timing:** Mid-session (after capabilities presentation)  
**Individual vs. Group:** Group discussion

**Setup:**
- Project the framework on screen or provide handouts
- Divide room into small groups (4-5 people per group)

**Part A: What AI Can Do Well (5 minutes)**

**Group Task:**
1. Review the list: "Generate code," "Suggest completions," "Explain code," "Translate languages," "Generate tests," "Document code," "Find bugs"
2. Discuss: "Which of these would be MOST valuable for your organization RIGHT NOW?"
3. Appoint one person to share your answer and why

**Debrief:**
- Ask each group: "What did you pick and why?"
- Capture on whiteboard
- Discuss: "Why did different teams pick different items?" (different needs, different tools)

---

**Part B: What AI Cannot Do (5 minutes)**

**Group Task:**
1. Review the list: "Understand business requirements," "Make architectural decisions," "Evaluate tradeoffs," "Debug complex systems," "Understand your codebase," "Replace judgment," "Maintain large context"
2. Discuss: "Which of these 'cannot do' items is MOST important for your team to remember?"
3. Appoint one person to share your answer and why

**Debrief:**
- Ask each group: "What's your biggest concern about AI limitations?"
- Capture on whiteboard
- Reinforce: "These are the areas where YOUR leadership is most valuable"

---

**Part C: Application to Your Organization (5 minutes)**

**Group Task:**
1. Think about your organization's primary AI use case (or intended use case)
2. Discuss: "Based on what AI can and can't do, is this a good fit?"
3. Identify: "What are the risks we should plan for?"
4. Prepare: "One specific recommendation for your leadership"

**Debrief:**
- Ask 2-3 groups: "What's your recommendation?"
- Capture on whiteboard
- Validate good thinking

---

### Exercise 3: Live Tool Evaluation (20 minutes)

**Location in Workbook:** Pages 110-130  
**Timing:** After evaluation matrix presentation  
**Individual vs. Group:** Large group walkthrough

**Setup:**
- Project the AI Tool Evaluation Matrix on screen
- Choose a real tool to evaluate as a group (GitHub Copilot recommended for dev shops)
- Have printed matrices available for note-taking

**Part A: Capabilities Assessment (5 minutes)**

**Facilitator:**
1. Display first criterion: "Addresses our specific needs"
2. Ask room: "On a scale of 1-5, how well does GitHub Copilot address your need to write code faster?"
   - Poll the room; see range of answers
   - Discuss: Why different scores?
   - Guide: "The right answer is what YOUR organization needs"
3. Repeat for other capability criteria
4. Calculate category score with room

**Teaching Tip:** "Notice that different organizations might score the same tool differently. That's correctâ€”there's no universal right answer."

---

**Part B: Security/Cost Assessment (5 minutes)**

**Facilitator:**
1. Jump to Security category (highest weight)
2. Ask: "What are your security concerns about sending code to GitHub Copilot?"
   - Listen for: Privacy, training data, compliance
   - Validate: These are real concerns
3. Review GitHub's data policy
4. Ask: "Given this policy, how would you rate security? 1-5?"
5. Do same quick exercise for Cost category

---

**Part C: Calculate Overall Score (5 minutes)**

**Facilitator:**
1. Sum up all categories on screen
2. Calculate weighted total
3. Announce: "This tool scored 72 out of 100"
4. Ask room: "What does this mean?"
   - "Potential fit, needs more evaluation" (from page 205)
   - "What gaps should we address?"
5. Discuss: "What would we need to learn more about before deciding?"

---

**Part D: Discussion (5 minutes)**

**Questions to Facilitate:**
- "How does this structured approach differ from your typical tool evaluation?"
- "What category did you weight most heavily? Why?"
- "What surprised you about this evaluation?"
- "How could you use this matrix in your organization?"

---

### Exercise 4: Small Group Tool Evaluation (30 minutes)

**Location in Workbook:** Pages 110-130, 504-516  
**Timing:** In final segment of session  
**Individual vs. Group:** Small groups (3-4 people)

**Setup:**
- Divide participants into groups
- Each group picks a different AI tool (or same tool for comparison)
- Provide printed evaluation matrices and calculators
- Set timer for 25 minutes of work

**Suggested Tool Options:**
- GitHub Copilot (code generation)
- ChatGPT / Claude (general AI)
- SonarQube (code analysis)
- Amazon CodeWhisperer (code generation)
- Tabnine (code completion)

**Part A: Group Evaluation (20 minutes)**

**Group Task:**
1. Divide evaluation matrix among group members
   - Person A: Capabilities
   - Person B: Cost & Vendor Stability
   - Person C: Security & Privacy
   - Person D: Support (or merge if fewer)
2. Each person scores their section with supporting notes
3. Combine scores into overall rating
4. Complete the "Decision" section (recommend Y/N and why?)
5. Discuss one group conclusion you want to share

**Instructor Role:**
- Circulate to each group
- Ask clarifying questions ("Why did you score this 3?")
- Push thinking ("What would move this to a 4?")
- Challenge assumptions ("Is this realistic?")

---

**Part B: Group Sharing (5 minutes)**

**Debrief Format:**
- Ask: "Would Group 1 share your results?" (2 minutes max)
  - Tool name and final score
  - Key strength
  - Key concern
  - Your recommendation
- Repeat with 1-2 other groups

**Discussion Questions:**
- "Did different groups score the same tool differently? Why?"
- "What was surprising about the results?"
- "Which evaluation category was hardest to assess?"

---

**Part C: Reflection (5 minutes)**

**Individual Reflection:**
Ask participants to jot down:
- One thing you'll use this matrix for
- One question remaining about evaluating tools
- One recommendation for your organization

---

## Assignments

### Homework Assignment: Day 1 Take-Home (Due Day 2)

**Location in Workbook:** Pages 518-526  
**Estimated Time to Complete:** 60-90 minutes  
**Purpose:** Deepens learning; prepares for Day 2 discussion

**Assignment Overview:**

**Select an AI tool** relevant to your organization (real tool you're considering or currently using)

**Deliverables:**

1. **Complete the AI Tool Evaluation Matrix** (pages 110-130)
   - Rate all five categories
   - Provide notes for each criterion
   - Calculate overall score
   - Document any research conducted

2. **Complete the ROI Calculator** (pages 212-222)
   - Calculate total annual costs
   - Estimate total annual benefits
   - Calculate ROI percentage
   - Identify payback period

3. **Bring completed work to Day 2**
   - In workbook OR printed copies
   - Your name on it (for group discussion)
   - Brief notes on your thought process

**Optional Enhancements:**
- Complete Risk Assessment Template (pages 351-447) if time allows
- Research tool security/privacy policies
- Identify 2-3 questions for Day 2 discussion

---

**Homework Grading Rubric (for instructor feedback):**

| Criteria | Excellent | Good | Needs Work |
|----------|-----------|------|------------|
| **Matrix Completion** | All categories scored with detailed notes | Most categories scored with notes | Incomplete or minimal notes |
| **ROI Calculation** | Realistic numbers with clear assumptions | Reasonable estimates | Numbers seem high or unrealistic |
| **Depth of Analysis** | Clear reasoning for scores; considers pros/cons | Basic scoring; limited analysis | Appears rushed or superficial |
| **Preparation for Discussion** | Ready to explain choices; has questions | Can explain main points | Hasn't thought through implications |

**Use in Class:**
- Collect at start of Day 2 (not formally graded, but reviewed)
- Use to facilitate homework review discussion (page 529)
- Identify common evaluation patterns
- Address misconceptions
- Celebrate good analysis

---

## Assessment and Evaluation

### Formative Assessment During Class

**Real-Time Checks for Understanding:**

1. **Opening Assessment (Individual)**
   - Use to gauge baseline knowledge
   - Monitor what concerns are mentioned
   - Adjust Day 1 pacing based on responses

2. **Reality Check Discussion (Group)**
   - Listen to what participants select as "most valuable"
   - Note which limitations resonate most
   - Correct misconceptions immediately

3. **Live Tool Evaluation (Whole Group)**
   - Ask room for scores on different criteria
   - Facilitate discussion on range of answers
   - Gauge whether evaluation process is clear

4. **Small Group Exercise**
   - Circulate and listen to conversations
   - Ask probing questions
   - Note misconceptions or gaps
   - Identify strong facilitators for future training

5. **Group Sharing**
   - Listen to explanations of scores
   - Ask follow-up questions
   - Notice reasoning quality
   - Validate good thinking

---

### Summative Assessment After Class

**Homework Review (Day 2):**

**Evaluation Criteria:**
1. **Completeness:** Did they finish the matrix and ROI calculator?
2. **Accuracy:** Are calculations correct? Are assumptions reasonable?
3. **Depth:** Did they show critical thinking or just fill in blanks?
4. **Preparation:** Are they ready to discuss their findings?

**Use in Facilitating Day 2 Discussion:**
- Identify interesting/contrasting evaluations to discuss
- Ask volunteering participants to share
- Note any common misconceptions to address
- Celebrate thorough work

---

### Post-Session Feedback Survey

**Questions to Ask (Distribute at End of Class):**

1. **Clarity:** How clear was the content today? (1-5 scale)
   - If low: What was confusing?

2. **Relevance:** How relevant is this content to your organization? (1-5)
   - If low: What topics would be more useful?

3. **Pace:** Was the pace appropriate? (Too slow / Good / Too fast)

4. **Most Valuable:** What was most valuable about today's session?

5. **Needs Improvement:** What could be improved?

6. **Ready for Day 2:** Do you feel ready to continue on Day 2? (Y/N)
   - If no: What questions remain?

7. **One Thing to Implement:** What's one thing you'll do differently as a result of today?

**Analysis of Results:**
- If multiple "too fast": Slow down Day 2 and add breaks
- If multiple "unclear": Revisit concepts with different examples
- If low "relevance": Adjust Day 2 examples to match feedback
- If many "questions remain": Plan extra Q&A time

---

## Additional Teaching Resources

### Key Talking Points to Memorize

**"The Three Pillars of AI Adoption"**
"Success requires three things:
1. **Technology** - The right tools for your needs
2. **Process** - How you use tools responsibly
3. **People** - Your team understands and wants to use them"

"Today focuses on #1 (Technology). Tomorrow we focus on #3 (People). Day 3 focuses on #2 (Process)."

---

**"The AI Tool Paradox"**
"Every tool says it does everything. Every tool can't. Your job is to:
1. Understand what it ACTUALLY does well
2. Understand what it ACTUALLY can't do
3. Match that to YOUR specific needs
4. Make decisions with data, not hype"

---

**"Red Flags When Evaluating Tools"**
"If you hear these, ask more questions:
- 'The AI understands context'â€”Does it? Test it.
- 'It works out of the box'â€”For everyone? Pilot it.
- 'Your code won't be used for training'â€”Get that in writing.
- 'It's secure'â€”What does that mean specifically? Get details.
- 'Everyone is using it'â€”Does that make it right for YOU?"

---

### Common Participant Questions & Answers

**Q: "Is GitHub Copilot secure? Can Microsoft see my code?"**
A: "Good question. By default, GitHub collects code to improve the service, but you can disable that. It's not used for training by default. Check their privacy policy for your specific concerns. That's exactly why we evaluate tools systematically."

**Q: "How much time can AI tools really save?"**
A: "Depends on your work. Routine coding? 20-40% faster. Complex architecture? Maybe 5%. The ROI calculator helps you estimate for YOUR work. Don't trust vendor claimsâ€”test it yourself in a pilot."

**Q: "Won't this put developers out of work?"**
A: "That's a fair concern. The data shows AI changes work, not eliminates jobs. Developers spend less time on boilerplate and more on complex problems. But organizations need to invest in training to make that transition smooth."

**Q: "What if the AI tool generates buggy code?"**
A: "Excellent question. That's why we're not going to blindly use AI output. Tomorrow we'll talk about code review processes for AI code. The code review becomes even more important, not less."

**Q: "Which AI tool should we choose?"**
A: "That's exactly what this framework is designed to help you figure out. There's no 'best' tool for everyone. There's the right tool for YOUR organization, YOUR tech stack, and YOUR needs."

---

### Suggested Presentation Slides (if using visual aids)

**Slide 1: Title**
- Evening 1: Signal vs. Noise - Cutting Through the AI Hype
- Subtitle: Making Evidence-Based AI Tool Decisions

**Slide 2: Today's Agenda**
1. Opening Assessment (Who are we? Where are we?)
2. Reality Check Framework (What can/can't AI do?)
3. Tool Categories (What AI tools exist?)
4. Evaluation Matrix (How do we decide?)
5. ROI & Risk (Is it worth it?)
6. Stakeholder Alignment (Who needs to agree?)

**Slide 3: The AI Landscape**
- Show chart of AI tool categories
- Emphasize: "Different problems, different tools"

**Slide 4: What AI Can Do Well**
- List with examples (one slide per category if presenting)
- Show before/after examples

**Slide 5: What AI Cannot Do**
- List with examples
- Emphasize: "This is where YOUR judgment matters"

**Slide 6: Evaluation Matrix Preview**
- Show 5 categories with weights
- Emphasize: "We evaluate systematically, not by hype"

**Slide 7: ROI Calculation**
- Show formula
- Show example calculation
- Emphasize: "Realistic numbers are critical"

**Slide 8: Stakeholder Communication**
- Map stakeholders and their concerns
- Emphasize: "Different messages for different groups"

**Slide 9: Tonight's Exercise**
- Explain group evaluation activity
- Set expectations: "You'll practice the full evaluation process"

**Slide 10: Homework**
- Explain homework assignment
- Set expectation: "Bring to Day 2 for discussion"

---

### Pre-Session Checklist

**1 Week Before:**
- [ ] Review all Day 1 content thoroughly
- [ ] Prepare slides and handouts
- [ ] Identify real-world case studies and examples
- [ ] Prepare projector/AV setup
- [ ] Notify participants of date/time/location

**3 Days Before:**
- [ ] Verify venue and AV setup
- [ ] Print all handouts
- [ ] Prepare name tags (if desired)
- [ ] Set up registration table
- [ ] Prepare backup materials

**1 Day Before:**
- [ ] Final AV system check
- [ ] Review participant list and adjust examples if possible
- [ ] Prepare facilitator notes in workbook
- [ ] Set up any video/audio materials
- [ ] Prepare example evaluations

**Day Of (2 hours before):**
- [ ] Arrive early to physical space
- [ ] Final AV test
- [ ] Arrange chairs and materials
- [ ] Test internet connectivity
- [ ] Prepare welcome remarks

---

## Conclusion and Transition to Day 2

**End-of-Session Talking Points:**

"You've learned:
1. What AI tools actually can and can't do
2. How to systematically evaluate tools against your needs
3. How to calculate ROI and understand risks
4. How to communicate with different stakeholders

This gives you a framework for making SMART decisions about AI adoptionâ€”not following hype, not rejecting too quickly, but making evidence-based choices."

**Setting Up Day 2:**

"Tomorrow, we shift focus from tools to people. We'll talk about:
- How different team members respond to AI (and what to do about it)
- How to adapt management practices for an AI-augmented team
- How to help your team grow in an AI world

Bring your homework evaluation. We'll discuss what you found."

**Contact and Next Steps:**

"Before you leave:
- Leave feedback on today's session (helps improve Day 2/3)
- Exchange contact information if you want to continue discussion
- Bring your homework tomorrow
- Submit any questions you want me to address tomorrow"

---

*End of Day 1 Teacher's Guide*

**Document Version:** 1.0  
**Last Updated:** January 2026  
**Next Review Date:** After first session delivery

---