# Day 2 Teacher's Guide
## Evening 2: Leading Humans in the Age of Copilots

**Course:** AI for Software Managers Workshop  
**Session Date:** January 21, 2026  
**Location:** Digital Campus Vorarlberg  
**Instructor:** Frank Blau  
**Duration:** Evening Session (3-4 hours recommended)

---

## Table of Contents
1. [Pre-Class Setup Instructions](#pre-class-setup-instructions)
2. [Learning Objectives](#learning-objectives)
3. [Tool and Software Requirements](#tool-and-software-requirements)
4. [Daily Schedule](#daily-schedule)
5. [Detailed Topic Guide](#detailed-topic-guide)
6. [Activities and Exercises](#activities-and-exercises)
7. [Assignments](#assignments)
8. [Assessment and Evaluation](#assessment-and-evaluation)
9. [Teaching Resources](#teaching-resources)

---

## Pre-Class Setup Instructions

### 48 Hours Before Class

**Instructor Preparation:**
- [ ] Review Day 1 homework submissions and assessment results
- [ ] Identify team composition patterns (resisters, skeptics, pragmatists, enthusiasts)
- [ ] Prepare case studies of real team adoption scenarios (have 2-3 examples ready)
- [ ] Practice role-play scenarios for 1:1 conversations
- [ ] Prepare video clips or case studies showing different developer reactions to AI
- [ ] Create sample team profiles for discussion exercises
- [ ] Review real performance review examples (with sensitive data redacted)

**Materials Preparation:**
- [ ] Print one "AI Anxiety Spectrum" reference card per participant
- [ ] Print one "Skills Evolution Map" per participant
- [ ] Print one "Performance Review Framework" template per participant
- [ ] Print one "Code Review 2.0 Checklist" per participant
- [ ] Print one "Team Health Dashboard" template per participant
- [ ] Print "1:1 Conversation Templates" (multiple copies for practice)
- [ ] Print career development planning worksheets
- [ ] Prepare flip charts or slides showing typical team dynamics

**Room Setup:**
- [ ] Arrange chairs for group discussion and role-play exercises
- [ ] Set up areas for small group breakouts (at least 2-3 spaces)
- [ ] Ensure whiteboards and markers available for dynamic notes
- [ ] Test any video equipment (if showing case studies)
- [ ] Prepare sticky notes and index cards for activities
- [ ] Set up timer or stopwatch for timed exercises

### 24 Hours Before Class

**Participant Communications:**
- [ ] Send reminder email reviewing Day 1 key concepts
- [ ] Ask participants to bring completed Day 1 homework for review
- [ ] Request participants think about current team challenges
- [ ] Provide parking/transportation reminders
- [ ] Confirm WiFi credentials are working

**Last-Minute Checks:**
- [ ] Review Day 1 participant feedback for content adjustments
- [ ] Verify all videos/clips play correctly
- [ ] Test presentation materials on projector
- [ ] Organize homework review materials by participant (if possible)

### Day of Class (Arrive 30 minutes early)

**Room Verification:**
- [ ] Test AV equipment one final time
- [ ] Check room temperature and lighting
- [ ] Arrange materials and handouts at tables
- [ ] Verify WiFi is functioning
- [ ] Set up name tags if continuing from Day 1

**Technical Setup:**
- [ ] Open presentation materials
- [ ] Have backup copies accessible
- [ ] Test any video playback
- [ ] Verify collaboration tools active (if using virtual breakout rooms)

---

## Learning Objectives

By the end of Day 2, participants will be able to:

### Knowledge Objectives
- **Understand** different developer reactions to AI adoption (resistance to enthusiasm)
- **Identify** which skills remain critical, which change, and which new skills are needed
- **Explain** how performance reviews and career development evolve with AI-augmented work
- **Recognize** the importance of code review practices when code is AI-generated
- **Articulate** team health indicators and adoption metrics
- **Understand** the business and human implications of AI adoption

### Skill Objectives
- **Apply** the AI Anxiety Spectrum to categorize team members and tailor approaches
- **Create** individual management plans for team members with different adoption levels
- **Conduct** difficult 1:1 conversations about AI using provided templates
- **Design** updated code review processes for AI-assisted development
- **Develop** performance review frameworks that reflect AI-augmented work
- **Build** career development plans incorporating new AI-era skills

### Attitude Objectives
- **Appreciate** the complexity of human factors in technology adoption
- **Recognize** that different developers need different approaches
- **Commit** to supporting team members through AI transition
- **Develop** empathy for adoption challenges across the team
- **Embrace** the need for continuous skill development in AI era

---

## Tool and Software Requirements

### Required Software/Platforms

| Tool | Version | Purpose | Validation |
|------|---------|---------|-----------|
| Spreadsheet Software | Excel 2019+ or Google Sheets | Team health dashboard, assessment scoring | Test: Create sample dashboard before class |
| Web Browser | Chrome/Firefox/Safari (latest) | Accessing reference materials | Test: Verify all links work |
| PDF Viewer | Built-in (Preview/Adobe) | Viewing templates and frameworks | Test: Open sample PDFs |
| Video Conferencing (Optional) | Zoom/Teams | For hybrid participants | Test: Audio/video quality |
| Collaboration Tool (Optional) | Miro, Mural, or Google Docs | Real-time team mapping | Test: Create sample board for practice |

### Hardware Requirements

| Item | Specification | Notes |
|------|---------------|-------|
| Projector | 2500+ lumens minimum | For 1920x1080 resolution |
| Screen/Whiteboard | 60"+ recommended | Flip charts acceptable alternative |
| Audio System | Microphone + speakers | For video clips and presentations |
| Laptops | For participants (optional) | For virtual breakout rooms |
| Flip Charts | 3-4 pads | For group discussions and team mapping |

### Reference Materials to Have Ready

1. **Printed Materials:**
   - AI Anxiety Spectrum framework
   - Skills Evolution Map with sample assessments
   - Performance Review Framework with examples
   - Code Review 2.0 Checklist
   - 1:1 Conversation Templates

2. **Digital Resources:**
   - Case studies of team adoption patterns
   - Real performance review examples (anonymized)
   - Sample team health dashboards
   - Career development path examples

3. **Assessment Tools:**
   - Team profile assessment worksheet
   - Individual management plan templates
   - Homework assignment checklist

---

## Daily Schedule

### Recommended Session Structure (3-4 hours)

| Time | Duration | Activity | Materials | Notes |
|------|----------|----------|-----------|-------|
| 6:00 PM | 5 min | Welcome & Day 1 Review | Projector | Recap key concepts |
| 6:05 PM | 15 min | Homework Review Debrief | Worksheets, flip chart | Share patterns from tool evaluations |
| 6:20 PM | 20 min | The AI Anxiety Spectrum | Slides, reference cards | Introduce 4 developer types |
| 6:40 PM | 15 min | Team Profile Mapping | Flip chart, markers | Participants identify team composition |
| 6:55 PM | 10 min | **BREAK** | Refreshments | Informal discussion time |
| 7:05 PM | 20 min | Skills Evolution Map | Slides, worksheets | What changes in the AI era |
| 7:25 PM | 15 min | Individual Development Plans | Templates, examples | Introduction and planning |
| 7:40 PM | 25 min | Performance Review Framework | Slides, templates | New metrics for AI era |
| 8:05 PM | 20 min | Code Review 2.0 Guidelines | Checklists, examples | How to review AI-generated code |
| 8:25 PM | 15 min | Team Health Dashboard | Templates | Monitoring adoption metrics |
| 8:40 PM | 30 min | Role-Play Exercise: 1:1 Conversations | Conversation cards, flip charts | Practice difficult conversations |
| 9:10 PM | 15 min | Career Development Discussion | Templates | Building long-term paths with AI |
| 9:25 PM | 10 min | Homework Assignment Review | Assignment sheet | Explain Day 2 homework |
| 9:35 PM | 5 min | Closing Remarks & Day 3 Preview | None | Set expectations for final day |

**Total Recommended Duration:** 3 hours 35 minutes  
**Flexible Options:** Can be expanded to 4 hours by extending breakout group time

---

## Detailed Topic Guide

### Topic 1: Homework Review & Context Setting (20 minutes)

**Objective:** Build on Day 1 learning and understand team adoption readiness

**Key Points to Convey:**
- Day 1 evaluations show real thinking about tool fit
- Patterns emerge across teams and tools
- Today shifts focus from tools to people
- Managing people is where success or failure happens

**Detailed Content Outline:**

**A. Quick Welcome and Day 1 Recap (5 minutes)**
- Thank participants for their effort on homework
- Highlight that they brought valuable frameworks to their organizations
- Emphasize: "Today's focus is on making these tools work for your teams"

**B. Homework Review Debrief (15 minutes)**
- Ask 3-4 volunteers to share their tool evaluations (2-3 minutes each)
- Guide questions:
  - "What surprised you most in your evaluation?"
  - "How did your team react to the frameworks?"
  - "Do you feel ready to move forward with this tool?"
  - "What questions do you still have?"
- Capture patterns on flip chart:
  - Common concerns across evaluations
  - Consensus on promising tools
  - Recurring questions

**Discussion Prompts:**
- "Which evaluation dimension was hardest to score? Why?"
- "How will you use these evaluations with your leadership?"
- "What obstacles do you anticipate in implementation?"

**Teaching Tips:**
- Affirm that different tool choices are appropriate for different organizations
- Emphasize that evaluation frameworks matter more than specific tool selections
- Use this to gauge team readiness level for Day 2 content

---

### Topic 2: The AI Anxiety Spectrum (35 minutes)

**Objective:** Help participants understand and manage different developer reactions to AI

**Why This Matters:**
- Teams are not monolithic—different developers react very differently
- One-size-fits-all approaches fail
- Understanding reactions allows targeted management strategies
- Early management prevents problems later

**Key Teaching Approach:**
- Use concrete examples from participant experiences
- Help participants recognize their own team members in the categories
- Emphasize empathy and understanding, not judgment
- Show that all categories can contribute positively

**Detailed Content Outline:**

**A. The Four Developer Archetypes (15 minutes)**

**1. The Resister (10-20% of developers)**

**Characteristics:**
- Refuses to try AI tools
- Sees AI as threat to job or expertise
- Believes AI cannot match human quality
- May feel their deep expertise is being devalued

**Why resistance happens:**
- Fear of job displacement
- Past negative experiences with technology
- Professional identity tied to expertise
- Concern about loss of control
- Legitimate concerns about code quality

**Management Approach:**
- Acknowledge concerns as valid (they're not irrational)
- Emphasize AI as tool, not replacement
- Show examples of AI limitations specifically
- Never force adoption
- Give them time and space
- Find pilot projects they can influence or opt out of
- Recognize their code review and quality expertise

**Red flags to watch:**
- Blocking adoption for whole team
- Publicly dismissing AI (creates doubt in others)
- Refusing to evaluate new approaches
- Using position to create resistance

**Success looks like:**
- Participation in limited evaluations
- Honest feedback on what doesn't work
- Gradual comfort with tool alongside humans

**B. The Skeptic (30-40% of developers)**

**Characteristics:**
- Willing to try but doubtful
- Concerned about quality and security
- Needs proof before committing
- Will adopt if convinced with evidence

**Why skepticism is healthy:**
- Forces rigorous evaluation
- Prevents blind hype adoption
- Raises important concerns about security/quality
- Most organizations benefit from skeptics

**Management Approach:**
- Provide data and evidence, not arguments
- Start with low-risk, easily reversible projects
- Address specific concerns directly with facts
- Let them discover benefits themselves
- Celebrate their questions as valuable
- Use them in pilot evaluation teams

**What to provide:**
- Peer experiences (other teams using tools successfully)
- Data on code quality metrics
- Security audit results
- ROI calculations from similar organizations

**Success looks like:**
- Cautious adoption after evidence presentation
- Strong code review practices for AI-generated code
- Valuable questions about implementation

**C. The Pragmatist (30-40% of developers)**

**Characteristics:**
- Sees AI as another tool
- Uses it when helpful, ignores when not appropriate
- Focused on getting work done
- Balanced and realistic perspective

**Why pragmatists are valuable:**
- Most productive early adopters
- Natural mentors for others
- Balanced approach prevents problems
- Represent the future state

**Management Approach:**
- Support their experimentation
- Share their positive experiences with team
- Use them as pilot participants and trainers
- Learn from their balanced approach
- Don't let them dismiss others' concerns

**Leverage them by:**
- Having them mentor other developers
- Having them share experiences in team meetings
- Using them in proof-of-concept projects
- Getting feedback on training approaches

**Success looks like:**
- Mature, balanced use of AI tools
- Willingness to share both successes and failures
- Continuous optimization of their workflow

**D. The Enthusiast (10-20% of developers)**

**Characteristics:**
- Excited about AI possibilities
- Early adopter
- May over-rely on AI
- Can be blind to limitations
- High energy and influence

**Why enthusiasm is double-edged:**
- Drives adoption forward
- Can generate momentum
- Can also cause problems if unchecked
- May miss important limitations

**Management Approach:**
- Channel enthusiasm productively
- Help them develop critical thinking about AI
- Watch for over-dependence patterns
- Use them to help train others
- Balance their optimism with reality-checks
- Ensure they maintain understanding of code

**Red flags to watch:**
- Never reviewing AI-generated code
- Assuming all AI output is correct
- Pushing adoption faster than organization ready
- Dismissing concerns about quality/security

**Success looks like:**
- Enthusiastic but critical evaluation of output
- Helping others adopt while maintaining quality standards
- Balanced use of tools alongside human skills

**B. Team Profile Mapping (20 minutes)**

**Exercise Setup:**
1. Have participants create a 2x2 matrix:
   - Horizontal axis: Resistance ← → Enthusiasm
   - Vertical axis: Individual ← → Team Average

2. Ask participants to map their team:
   - Plot each team member
   - Identify clusters
   - Note outliers

3. Facilitated discussion:
   - "Who are your resisters, and why?"
   - "Who drives adoption forward?"
   - "What's the overall team profile?"
   - "Where do the blockers sit?"

**Teaching Tips:**
- Normalize that all types are present in good teams
- Show that distribution affects management strategy
- Help participants see that team composition changes over time
- Emphasize that understanding is first step to managing

---

### Topic 3: Skills Evolution Map (25 minutes)

**Objective:** Help participants understand what skills matter in AI-augmented development

**Why This Matters:**
- Skills gaps create anxiety and resistance
- Not all skills are equally affected
- New skills are emerging that weren't needed before
- Career development must evolve with technology

**Key Teaching Approach:**
- Emphasize continuity (many skills remain critical)
- Show that change is gradual, not sudden
- Help participants see career opportunities, not just threats
- Provide concrete skill development pathways

**Detailed Content Outline:**

**A. Skills That Remain Critical ("Analog Awareness")**

These human skills become MORE valuable with AI, not less:

1. **System Design and Architecture**
   - Why: Someone must decide WHAT AI works on vs. what needs human judgment
   - Examples: Choosing microservices vs. monolith, deciding caching strategy
   - Development: Build case study experiences, architecture reviews with team

2. **Understanding Business Requirements**
   - Why: AI needs clear specifications; business judgment remains human domain
   - Examples: What constitutes "success"? What are acceptable tradeoffs?
   - Development: Practice translating business needs to technical specs

3. **Problem Decomposition**
   - Why: Breaking complex problems into pieces is human expertise
   - Examples: "How should we modularize this? What are the boundaries?"
   - Development: Code reviews, architecture discussions, mentoring

4. **Code Review and Quality Judgment**
   - Why: Understanding whether code is right requires human judgment
   - Examples: "Does this solve the business problem? Are there better approaches?"
   - Development: Peer learning, diverse review perspectives

5. **Security and Performance Thinking**
   - Why: AI may miss non-obvious security issues or performance implications
   - Examples: "Could this be vulnerable? Is the algorithm efficient enough?"
   - Development: Security training, performance profiling exercises

6. **Debugging Complex Issues**
   - Why: System interactions are too complex for current AI to understand
   - Examples: "Why is this failing only in production? What's causing the cascade?"
   - Development: Incident response participation, complex problem solving

7. **Team Communication and Mentoring**
   - Why: Human factors in adoption matter more than tooling
   - Examples: Helping team members learn, resolving conflicts, building culture
   - Development: Mentoring programs, communication skills training

**B. Skills That Are Changing**

These skills are evolving but remain important:

1. **Code Writing (More Guiding Than Typing)**
   - Old skill: Fast typing, remembering syntax
   - New skill: Writing clear specifications for AI, reviewing AI output, knowing when AI can help
   - Development: Practice writing clear requirements, AI prompt engineering

2. **Syntax Memorization (Less Important)**
   - Why: AI can generate syntax correctly
   - What matters: Understanding patterns and knowing what code should do
   - Development: Learn language concepts deeply, not just syntax

3. **Boilerplate Code Creation (Largely Automated)**
   - Why: This is exactly what AI excels at
   - What matters: Understanding what boilerplate is needed and where
   - Development: Focus on understanding why patterns exist

4. **Documentation Writing (AI-Assisted)**
   - Why: AI can generate first draft documentation
   - What matters: Reviewing for accuracy, adding context AI misses
   - Development: Learn to review and enhance AI-generated documentation

5. **Test Writing (AI-Assisted)**
   - Why: AI can generate many test cases
   - What matters: Defining what's important to test, edge cases for business logic
   - Development: Strengthen testing thinking, not just code generation

**C. New Skills Needed (Digital Competency)**

Skills that didn't matter before but matter now:

1. **Prompt Engineering**
   - What: Ability to ask AI tools effectively and clearly
   - Why: "Garbage in, garbage out" applies to AI prompts
   - How to develop:
     - Practice writing clear specifications
     - Experiment with different phrasing
     - Share effective prompts with team
     - Learn what works for different AI tools

2. **AI Output Evaluation**
   - What: Quickly spotting errors, gaps, and appropriate use
   - Why: AI is 90% right on average, but the 10% errors matter
   - How to develop:
     - Code review practice with AI-generated code
     - Learning common AI failure modes
     - Understanding your domain well

3. **Tool Selection**
   - What: Knowing which AI tool for which job
   - Why: GitHub Copilot ≠ ChatGPT ≠ Claude
   - How to develop:
     - Hands-on experimentation
     - Building evaluation framework
     - Learning from team experiences

4. **Hybrid Workflows**
   - What: Integrating human and AI work smoothly
   - Why: Best results come from combining strengths
   - How to develop:
     - Reflecting on your workflow
     - Experimenting with AI at different stages
     - Learning from pragmatists on your team

5. **AI Limitations Awareness**
   - What: Knowing what AI can't do and when not to use it
   - Why: Prevents over-reliance and quality issues
   - How to develop:
     - Studying documented AI failures
     - Experiencing limitations firsthand
     - Building mental models of AI capabilities

**D. Skills Assessment Exercise (10 minutes)**

**Setup:**
Provide skill assessment worksheet with dimensions:
- Analog Awareness (system design, requirements, debugging, etc.)
- Hybrid Skills (code writing, documentation, testing, etc.)
- Digital Competency (prompt engineering, AI evaluation, etc.)

**Activity:**
1. Participants rate team's current capability (1-5 scale)
2. Identify target levels for 6-12 months
3. Note the gaps
4. Group discussion: "What's your biggest skill gap?"

**Teaching Tips:**
- Normalize that everyone has gaps
- Show that growth is possible
- Emphasize that different roles need different emphasis
- Connect skills to career development opportunities

---

### Topic 4: Performance Review Framework (25 minutes)

**Objective:** Help participants update how they evaluate developer performance with AI in the mix

**Why This Matters:**
- Traditional metrics (lines of code) become meaningless
- Performance evaluation affects motivation, retention, advancement
- New framework must reward right behaviors
- Transitions are difficult without clear expectations

**Key Teaching Approach:**
- Show why old metrics fail
- Present concrete new metrics with examples
- Help participants think about their own context
- Provide template for customization

**Detailed Content Outline:**

**A. Why Traditional Metrics Break Down (5 minutes)**

**Old Metric Problems:**
- **Lines of Code Written:** AI writes code; that doesn't mean developer is productive
- **Speed of Coding:** AI is faster; developer's judgment is what matters
- **Bug Rates Alone:** May increase if developer isn't reviewing AI output properly
- **Feature Completion:** Doesn't capture code quality or maintainability

**The Problem:**
If you still measure success by volume, developers will:
- Accept AI output without review (speed without quality)
- Generate code when human thinking would be better
- Not develop judgment skills

**The Solution:**
Shift from volume metrics to outcome metrics.

**B. New Performance Indicators (15 minutes)**

**1. Quality Over Quantity**
- **Code that passes review on first submission:** Indicates thorough thinking before writing
- **Number of bugs reaching production:** Did the developer catch issues? Was AI output verified?
- **Test coverage and quality:** Appropriate tests for business logic, not just code coverage
- **Documentation clarity:** Can other developers understand the code?
- **Architecture decisions quality:** Did they make good structural choices?

**How to measure:**
- Code review records
- Bug tracking system
- Test reports
- Team feedback
- Architect assessment

**2. Problem Solving**
- **Ability to decompose complex problems:** Can they break down difficult issues?
- **Solution creativity and elegance:** Do they find good approaches?
- **Understanding of business context:** Can they explain why they chose this solution?
- **Speed of debugging:** When things break, how quickly do they find root cause?

**How to measure:**
- Observation during incident response
- Architecture review discussions
- Code review comments
- Problem-solving exercises

**3. AI Tool Mastery**
- **Effective use of AI tools:** Getting value from AI assistance
- **Ability to evaluate AI output critically:** Not blindly accepting AI output
- **Knowing when NOT to use AI:** Using judgment about when AI helps vs. hinders
- **Teaching others about AI tools:** Helping team learn effective use

**How to measure:**
- 1:1 conversations about their process
- Code review observations
- Peer feedback
- Mentoring activities

**4. Collaboration**
- **Code review quality and helpfulness:** Do they give useful feedback?
- **Knowledge sharing:** Do they help others learn?
- **Mentoring effectiveness:** Can others learn from them?
- **Team communication:** Do they communicate clearly and often?

**How to measure:**
- Peer feedback
- Code review records
- Mentoring relationships
- Team surveys

**5. Continuous Learning**
- **Adaptation to new tools:** Learning new AI tools as they emerge
- **Skill development:** Growing in new areas
- **Staying current:** Understanding AI evolution and capabilities
- **Sharing learnings:** Teaching team what they learn

**How to measure:**
- Skills assessment over time
- Training participation
- Articles/tools they share with team
- Technical discussions in team meetings

**C. Performance Review Template in Action (5 minutes)**

**Show example review:**
- Demonstrate how to fill out new framework
- Show evidence-based approach
- How it differs from old LOC-based review
- How it reflects AI-era work

**Teaching Tips:**
- Walk through specific example
- Show what "strong" and "developing" look like
- Help participants see how to gather evidence
- Emphasize fairness and consistency

---

### Topic 5: Code Review 2.0 Guidelines (20 minutes)

**Objective:** Help participants update code review practices for AI-assisted development

**Why This Matters:**
- AI-generated code requires different review lens
- Traditional code review isn't sufficient
- AI code has different failure modes than human code
- Quality gates must adjust for mixed human/AI development

**Key Teaching Approach:**
- Show concrete code review examples (both good and problematic AI output)
- Provide checklist for consistent application
- Help participants think about their own code review culture
- Address concerns about increased review burden

**Detailed Content Outline:**

**A. Standard Code Review Still Applies (8 minutes)**

**Traditional checklist items that remain critical:**

1. **Functionality:** Does the code do what it's supposed to do?
2. **Correctness:** Is the logic actually correct?
3. **Edge Cases:** Are boundary conditions handled? What if input is empty, huge, or malformed?
4. **Error Handling:** Are errors caught and managed appropriately?
5. **Testing:** Are there adequate tests? Do they pass?
6. **Documentation:** Is the code well documented?
7. **Style:** Does it follow team standards?
8. **Security:** Are there obvious security vulnerabilities?
9. **Performance:** Is it efficient? Did they add unnecessary complexity?
10. **Maintainability:** Can others understand and modify it?

**Teaching Point:**
These don't change just because AI helped write the code.

**B. Additional Checks for AI-Generated Code (7 minutes)**

**New lens for AI-assisted development:**

1. **Understanding:** Does the developer understand the code they're submitting?
   - Red flag: "I'm not sure what this does" or "I didn't really review it"
   - How to assess: Ask developer to explain difficult sections

2. **Over-Engineering:** Is the solution unnecessarily complex?
   - AI often generates feature-rich solutions when simple would work
   - Red flag: Many lines of code for simple problem
   - How to assess: Can the solution be simplified?

3. **Context Awareness:** Does it fit the existing codebase?
   - AI may not understand your architectural patterns
   - Red flag: Code doesn't match your style or conventions
   - How to assess: Compare with similar code in codebase

4. **Hidden Assumptions:** Are there unstated assumptions?
   - AI may assume patterns you don't use
   - Red flag: Code uses features or libraries you haven't adopted
   - How to assess: Would new developer understand why this was chosen?

5. **License Issues:** Could there be licensing problems?
   - If AI trained on open source, code might have license implications
   - Red flag: Check vendor policies on code origin
   - How to assess: Do you understand data origin of the AI tool?

6. **Outdated Patterns:** Is it using old approaches?
   - AI might generate outdated solutions if training data includes older code
   - Red flag: Using deprecated methods or patterns
   - How to assess: Is there a better, more modern way?

7. **Copy-Paste Errors:** Inconsistencies or duplicated sections?
   - AI sometimes generates similar code multiple times
   - Red flag: Duplicated logic that could be extracted
   - How to assess: Could this be refactored?

**C. Code Review Conversation (5 minutes)**

**Key questions to ask developer in review:**

For AI-assisted code specifically:
1. "Can you explain how this code works?"
2. "Did AI help write this, or did you write it yourself?"
3. "Did you test all the edge cases?"
4. "Why did you choose this approach over alternatives?"
5. "What alternatives did you consider?"

**What good answers look like:**
- Clear explanation of logic
- Honest about AI involvement
- Evidence of edge case testing
- Thoughtful reasoning about tradeoffs

**Red flags in responses:**
- Vague explanations
- Dishonesty about AI usage
- "I didn't really review it"
- Can't explain tradeoffs

**D. Team Code Review Standards (5 minutes)**

**Help participants develop standards:**

Exercise: "What code review standards will you establish for your team?"

Areas to standardize:
- Must AI-generated code be flagged in PR?
- Required approval processes for AI-generated vs. human-written
- Who is qualified to review AI output?
- What happens if developer can't explain code?
- How do you handle suspected code they don't understand?

**Teaching Tips:**
- Emphasize that review is not about blame
- Show how this maintains quality standards
- Help teams see this as professional development
- Normalize questions about AI usage and understanding

---

### Topic 6: Team Health Dashboard & 1:1 Conversations (45 minutes)

**Objective:** Help participants monitor adoption and address individual concerns

**Why This Matters:**
- Adoption isn't linear—needs ongoing monitoring
- Individual concerns require individual conversations
- Early detection of problems allows intervention
- Creates safety for honest feedback

**Key Teaching Approach:**
- Show concrete metrics that matter
- Practice difficult conversations with role-play
- Normalize that conversations are essential to success
- Provide structured templates and talking points

**Detailed Content Outline:**

**A. Team Health Dashboard (15 minutes)**

**Purpose:**
- Early warning system for adoption problems
- Data-driven decisions about intervention
- Tracking progress over time
- Understanding impact on team

**Key Metrics to Track (Monthly):**

**Team-Level Indicators:**
- Team morale (1-5 scale survey)
- Average task completion time (trend up = positive)
- Code review cycles needed (fewer = more efficient)
- Production bugs (trend should be stable or down)
- Developer satisfaction (1-5 scale)
- AI tool usage rate (% of developers, % of code)

**Individual Tracking:**
- Which developers using AI?
- Comfort level with AI tools (1-5)
- Concerns or blockers expressed
- Quality of AI-assisted code

**Qualitative Indicators:**
- How many developers in each category? (Resistant, skeptical, pragmatist, enthusiast)
- What concerns are being expressed?
- What actions have been taken?
- What's needed next?

**Red Flags to Watch:**
- Declining team morale
- Increased production bugs
- Widening gaps between developer usage
- Repeated concerns not being addressed
- Developers over-relying on AI

**Green Flags (Things Going Well):**
- Stable or improving morale
- Increased adoption among skeptics
- Pragmatists mentoring others
- Enthusiasts becoming more balanced
- Fewer quality concerns

**B. One-on-One Conversation Framework (30 minutes)**

**Why individual conversations matter:**
- Same situation is different for each person
- Trust requires individual attention
- Concerns need personalized responses
- Development happens in relationships

**Three Critical Conversations:**

**Conversation 1: The Resistant Developer**

**When to have it:**
- After introductory AI tools training
- If they haven't tried tools after reasonable time
- If they're discouraging others

**Opening:**
- "I've noticed you haven't been using the new AI tools we introduced. I wanted to understand your perspective."
- Listen without defending

**Listen for:**
- Specific concerns about job security
- Quality worries (legitimate and illegitimate)
- Skill preservation fears
- Past negative technology experiences
- Philosophical objections

**Key messages to convey:**
- Their expertise is valued and needed
- AI is a tool, not a replacement
- No one is forced to adopt
- We want their honest feedback (not just compliance)
- Their concerns are worth addressing

**Don't say:**
- "Everyone else is using it"
- "You'll fall behind"
- "It's the future whether you like it or not"
- Dismissive comments about their concerns

**Closing:**
- "What would make you more comfortable exploring these tools, even just to evaluate them?"
- Offer low-pressure options (maybe not their main project)
- Set a check-in date

**Follow-up:**
- Actually listen to their conditions
- Check in on progress
- Recognize small steps forward

**Success looks like:**
- Willingness to try in limited way
- Honest feedback on what doesn't work
- Gradual movement toward pragmatism

**Conversation 2: The Over-Reliant Developer**

**When to have it:**
- When they're submitting code they can't explain
- If code quality is declining
- If they're over-confident about AI correctness
- During code review when patterns emerge

**Opening:**
- "I wanted to talk about how you're using AI tools in your work."
- Frame as development conversation, not criticism

**Listen for:**
- Their understanding level of submitted code
- Testing and validation practices
- How much time they're spending reviewing vs. generating
- Actual vs. perceived productivity
- Confidence level (over or appropriately?)

**Key messages to convey:**
- AI is a productivity tool, not a thinking replacement
- Understanding code is non-negotiable
- Quality matters more than speed
- Balance is important and healthy
- We value critical thinking

**Don't say:**
- "You're being lazy"
- "You're not a real developer if you use AI"
- "You should be able to do this yourself"
- Dismissive comments about their speed

**Closing:**
- "What's one way you can verify AI-generated code more thoroughly?"
- Specific commitments (e.g., "I'll manually test all edge cases")
- Regular check-ins on practices

**Follow-up:**
- Observe if behavior changes
- Praise improved practices
- Keep expectations clear

**Success looks like:**
- More careful review of AI output
- Better testing practices
- Maintained or improved code quality
- More balanced tool usage

**Conversation 3: Career Development with AI**

**When to have it:**
- Regularly (at least annually)
- When developer expresses concern about career
- During performance reviews
- When developmental opportunities arise

**Opening:**
- "Let's talk about your career growth and how AI tools are changing what matters."
- Frame as forward-looking opportunity, not threat

**Topics to cover:**
- Skills that remain valuable (and getting more valuable)
- New skills they need to develop
- Career path options with AI
- Growth opportunities

**Questions to ask:**
- "Where do you see your career in 2-3 years?"
- "What skills do you want to develop?"
- "How can AI tools help you grow?"
- "What concerns do you have about your career in AI era?"
- "What would help you feel more secure?"
- "What do you want to become known for?"

**Key messages to convey:**
- Your human skills make you valuable
- New skills will expand your opportunities
- You're not being replaced; you're evolving
- We'll support your development
- Your career is important to us

**Development planning:**
- Identify skill gaps
- Create learning opportunities
- Recognize progress
- Connect to opportunities

**Success looks like:**
- Clearer sense of career direction
- Commitment to skill development
- Reduced anxiety about future
- More engaged in learning

**C. Role-Play Exercise (30 minutes)**

**Setup and Running the Exercise:**

1. **Pair participants** (even numbers; if odd, facilitate one yourself)

2. **Assign roles:**
   - Person A: Manager (50% get Resistant developer scenario, 50% get Over-reliant)
   - Person B: Developer (opposite scenario)
   - Swap halfway through

3. **Give 5 minutes for person in developer role to get into character:**
   - Scenario cards describe the situation
   - They should think about their perspective
   - May prep a few things they want to say

4. **Role-play conversation (5-7 minutes)**
   - Manager initiates conversation
   - Try to follow template but adapt naturally
   - Practice active listening
   - Practice difficult feedback

5. **Debrief in pairs (3 minutes):**
   - "What went well?"
   - "What felt difficult?"
   - "What would you do differently?"

6. **Swap roles and repeat** with different scenario

7. **Large group debrief (10 minutes):**
   - Ask 3-4 pairs: "What did you learn?"
   - "What was hardest?"
   - "Any surprises?"
   - Share common themes
   - Reinforce key talking points

**Teaching Tips:**
- Let imperfection happen—that's learning
- Normalize awkwardness in these conversations
- Emphasize listening over perfect responses
- Celebrate efforts to understand
- Share your own experiences with difficult conversations

---

### Topic 7: Career Development Planning (15 minutes)

**Objective:** Help participants support long-term career growth in AI era

**Why This Matters:**
- Career development is retention driver
- Uncertainty about future leads to resignation
- Supporting growth shows genuine care
- Individual development plans create shared commitment

**Detailed Content Outline:**

**A. Framing Career Development (5 minutes)**

**Key messages:**
- AI doesn't eliminate careers; it transforms them
- New opportunities emerge for those who develop
- Skills remain valuable but application changes
- Your role as manager is to support evolution

**B. Individual Development Plan Components (10 minutes)**

**Template walkthrough (use volunteer participant as example):**

**1. Strengths Assessment**
- What's this developer great at now?
- What do others count on them for?
- How do these transfer to AI era?

**2. Development Areas**
- Where do they struggle?
- What skills are emerging that they lack?
- What's their motivation to develop?

**3. AI-Era Skills Assessment**
- AI tool usage and comfort
- Prompt engineering capability
- Critical evaluation of AI output
- Architecture and design thinking
- System design and performance

**4. Six-Month Development Plan**

Quarter 1 (Months 1-3):
- 1-2 specific learning goals
- Activities to support goals (courses, projects, mentoring)
- Success measures

Quarter 2 (Months 4-6):
- Build on Quarter 1
- More complex or advanced goals
- Broader application

**5. Support Needed**
- Training budget
- Time for learning
- Mentoring/coaching
- Project assignments
- Conference attendance
- Certifications

**6. Success Measures**
- How will you know they're progressing?
- Concrete evidence (completed training, applied skills, feedback)

**Teaching Tips:**
- Make this collaborative, not prescribed
- Listen to what they want to develop
- Connect their development to organizational needs
- Regular check-ins on progress
- Celebrate progress

---

## Activities and Exercises

### Exercise 1: Team Profile Mapping (20 minutes)

**Objective:** Help participants understand their team composition and dynamics

**Materials Needed:**
- Flip chart paper
- Markers
- AI Anxiety Spectrum reference card (one per person)

**Setup:**
1. Explain the four developer types briefly
2. Show example 2x2 matrix (Resistance/Enthusiasm vs. Individual/Team)

**Activity:**
1. **Individual Reflection (5 minutes)**
   - Participants list their team members by name
   - Assign each to a category
   - Mark outliers or difficult-to-categorize people

2. **Group Exercise (10 minutes)**
   - Facilitate whole group building a composite team map
   - Ask: "What's your team profile?" (from 3-4 different organizations)
   - Draw on flip chart showing patterns across organizations
   - Note outliers and clusters

3. **Discussion (5 minutes)**
   - "What does your distribution tell you?"
   - "How does this affect your adoption strategy?"
   - "Who's going to be your champions?"
   - "Who needs special attention?"

**Debrief:**
- Patterns: Balanced teams, teams heavy on resisters, teams heavy on enthusiasts
- Impact: Different strategies needed for different profiles
- Action: Start identifying who to talk to first

---

### Exercise 2: Skills Gap Assessment (15 minutes)

**Objective:** Help participants identify development priorities for their teams

**Materials Needed:**
- Skills Evolution Map template (one per participant)
- Pencils
- Optional: Colored markers

**Activity:**
1. **Explanation (3 minutes)**
   - Walk through the three skill categories
   - Show examples of each

2. **Individual Assessment (7 minutes)**
   - Participants rate their team's current capability (1-5 scale) across:
     - Analog Awareness skills (system design, requirements, etc.)
     - Hybrid Skills (code writing, testing, etc.)
     - Digital Competency (prompt engineering, AI evaluation, etc.)
   - Identify their biggest gaps

3. **Discussion (5 minutes)**
   - "What's your biggest skill gap?"
   - "How will you address it?"
   - Share patterns across group

**Debrief:**
- Most teams identify prompt engineering and AI evaluation as biggest gaps
- Analog skills are often neglected but matter most
- Digital competency is new and needs intentional development

---

### Exercise 3: Code Review Role-Play (30 minutes)

**Objective:** Help participants practice updated code review discussions

**Materials Needed:**
- Code Review 2.0 checklist (one per person)
- Sample code snippets (provided or from participant codebases)
- 1:1 Conversation cards

**Setup:**
1. Show example AI-generated code with issues
2. Walk through what to notice
3. Practice code review conversation

**Activity:**
1. **Demonstration (10 minutes)**
   - Instructor shows problematic code
   - Walks through using Code Review 2.0 checklist
   - Shows how to ask clarifying questions
   - Models checking understanding

2. **Practice in Small Groups (15 minutes)**
   - Groups of 3-4
   - One person acts as code author, one as reviewer
   - Others observe and provide feedback
   - Swap roles so everyone practices

3. **Large Group Discussion (5 minutes)**
   - "What was hard?"
   - "What felt natural?"
   - "Key takeaways?"

---

### Exercise 4: 1:1 Conversation Role-Play (30 minutes)

**Objective:** Help participants practice difficult conversations

**Materials Needed:**
- 1:1 Conversation scenario cards
- Note-taking space
- Optional: Timer

**Setup:**
1. Explain the three types of conversations
2. Show example conversation flow

**Activity:**
1. **Pair participants** (A and B roles rotate)

2. **Scenario 1: Resistant Developer (5-7 min conversation + 3 min debrief)**
   - One participant plays manager
   - Other plays developer who hasn't tried AI tools
   - Manager uses template to start conversation
   - Focus on listening and understanding

3. **Scenario 2: Over-Reliant Developer (5-7 min conversation + 3 min debrief)**
   - Swap roles
   - Developer is using AI but not reviewing carefully
   - Manager addresses quality concerns constructively

4. **Large Group Debrief (10 minutes)**
   - Ask pairs: "What went well?"
   - "What surprised you?"
   - "Any new insights?"
   - Share patterns and learning

**Teaching Tips:**
- Normalize imperfection
- Celebrate attempts at difficult conversations
- Show how listening comes first
- Emphasize compassion and understanding

---

## Assignments

### Homework Assignment: Day 2 Take-Home (Due Day 3)

**Assignment: Team Assessment and Individual Development Plan**

**Part 1: Team Assessment (30 minutes)**
- Map your team to AI Anxiety Spectrum
  - Identify resisters, skeptics, pragmatists, enthusiasts
  - Note outliers or people you're unsure about
  
- Create Team Health Dashboard baseline
  - Record current metrics (morale, productivity, adoption rate)
  - Note any concerns
  
- List top 3 team members who need 1:1 conversations
  - Reason for conversation
  - Planned approach

**Part 2: Individual Development Plan (30 minutes)**
- Select one team member to create a development plan for
- Use provided template to complete:
  - Strengths and development areas
  - AI-era skills assessment
  - 6-month development goals
  - Support they'll need

**Part 3: Planning Your Conversations (30 minutes)**
- Write notes for 1:1 with resistant or over-reliant developer
- Prepare opening statement
- Anticipate concerns and responses
- Plan follow-up actions

**Submission:**
- Bring completed homework to Day 3
- Be prepared to discuss: "What was the hardest part?"

**Grading Rubric:**

| Dimension | Excellent | Good | Developing |
|-----------|-----------|------|-----------|
| Team Mapping | Accurate identification; good insight into dynamics | Clear identification of types; reasonable analysis | Basic categorization; limited insights |
| Development Plan | Detailed, realistic, well-connected to skills/goals | Complete, reasonable goals, realistic support | Missing elements or unrealistic expectations |
| Conversation Planning | Thoughtful approach; addresses likely concerns | Solid plan with key talking points | Basic outline; missing key elements |

**Usage Notes for Day 3:**
- Homework provides baseline for final day's governance discussion
- Team assessments show adoption readiness
- Development plans support Day 3 career/retention conversation

---

## Assessment and Evaluation

### Formative Assessment (During Class)

**Check for Understanding:**

**After AI Anxiety Spectrum section:**
- Ask: "Can someone describe what you'd expect from each type?"
- Look for: Recognition of different perspectives, not judgment

**After Skills Evolution section:**
- Small poll: "What skill gap concerns you most?"
- Look for: Understanding that skills are evolving, not disappearing

**After Code Review section:**
- Ask: "What's different when reviewing AI-generated code?"
- Look for: Recognition of both traditional and AI-specific factors

**During role-plays:**
- Observe quality of listening in conversations
- Note use of templates and key talking points
- Provide real-time feedback and modeling

### Summative Assessment (Homework)

**Day 2 Homework Assessment**

**Team Mapping:**
- Are categorizations accurate? (observed during warm-up conversations)
- Do insights show genuine understanding of team dynamics?
- Is analysis practical or theoretical?

**Development Plan:**
- Does plan address real gaps?
- Are goals specific and measurable?
- Is support realistic?
- Does plan align with organizational needs?

**Conversation Planning:**
- Shows thoughtfulness about individual concerns?
- Plans for listening and understanding?
- Identifies follow-up actions?

### Feedback and Participation

**Track:**
- Engagement in discussions
- Quality of role-play participation
- Insights shared about their teams
- Questions asked

**Provide:**
- Real-time feedback during exercises
- Coaching on conversation skills
- Validation of difficult team situations
- Encouragement for planned actions

### Post-Session Survey

**Optional feedback questions:**
- "How confident do you feel having 1:1 conversations about AI?" (1-5 scale)
- "What's one thing you'll do differently with your team?"
- "What would have been helpful to explore more?"
- "How is this workshop helping you manage the transition?"

---

## Teaching Resources

### Key Talking Points

**On Adoption Diversity:**
- "Not everyone adopts change at the same pace. That's normal and healthy."
- "Your job isn't to convert resisters; it's to help them understand and make choices."

**On Human Skills:**
- "AI makes human judgment more valuable, not less."
- "The skills that made good architects, good reviewers, and good leaders don't change."

**On Conversations:**
- "These conversations are going to feel awkward at first. That's okay."
- "The goal is understanding, not persuasion."
- "Listen first, explain your perspective second."

**On Career Development:**
- "Your developers' careers matter. Show them you care."
- "Growth opportunities exist for those who develop new skills."
- "AI is a tool for acceleration, not replacement."

### Common Q&A with Suggested Answers

**Q: "What if someone just refuses to use AI tools?"**
A: "That's their choice, and it's okay. Your job is to explain why it matters and ensure they understand what they might miss. But force doesn't work. Start with understanding their concerns."

**Q: "How do I know if someone over-relies on AI?"**
A: "Watch for code they can't explain, declining code quality, or statements like 'AI wrote it.' Have the conversation focusing on understanding and quality, not blame."

**Q: "How much time should I spend on 1:1s about AI?"**
A: "Start with 20-30 minutes per person. These aren't performance reviews; they're development conversations. Schedule follow-ups as needed."

**Q: "What if my team is small and I can't afford to have resisters?"**
A: "Start by understanding their concerns. Often resisters have valid points about quality or security. Listen to their expertise; don't dismiss it."

**Q: "How do I update performance reviews mid-year?"**
A: "You don't need to change everything immediately. Focus on outcomes and quality. Gradually introduce new metrics. Transparency helps—explain what's changing and why."

**Q: "What if someone's career is truly at risk?"**
A: "Be honest and supportive. Show them what skills matter and help them develop them. Support them in exploring new roles or organizations if needed. Care matters."

### Presentation Slide Suggestions

**Slide Deck Topics:**
1. Welcome and Day 1 Review
2. The Four Developer Archetypes (one slide each with characteristics)
3. Team Profile Mapping (show 2x2 matrix)
4. Skills That Remain Critical
5. Skills That Are Changing
6. New Skills Needed
7. Why Old Performance Metrics Break
8. New Performance Indicators (with examples)
9. Code Review 2.0 Differences
10. Team Health Dashboard Metrics
11. 1:1 Conversation Framework
12. Career Development Planning
13. Day 3 Preview

### Pre-Session Checklist

- [ ] Review Day 1 homework submissions (if available)
- [ ] Prepare team profile examples from your own experience
- [ ] Practice role-play conversations (especially difficult ones)
- [ ] Set up breakout spaces for small group exercises
- [ ] Test any video clips or case studies
- [ ] Print all handouts and worksheets
- [ ] Prepare flip charts with starting prompts
- [ ] Brief any co-instructors on their roles
- [ ] Plan timing carefully—this session has a lot of content
- [ ] Have backup materials if time runs short or long

---

## Closing Thoughts

Day 2 is emotionally intensive because it focuses on real human concerns. Your role is to normalize the transitions, provide practical frameworks, and show genuine care for your participants' teams. The conversations they practice today will happen many times over—make sure they feel equipped and supported.

End the session with confidence that they have frameworks, templates, and practice to handle the transition. Day 3 will focus on the organizational and governance structures to support all this human work.

**Remember:**
- Different developers need different approaches
- Listening comes first
- Human skills remain most valuable
- Development and support matter
- You're helping people navigate real change

---

**End of Day 2 Teacher's Guide**